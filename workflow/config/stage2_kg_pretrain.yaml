hydra:
  run:
    dir: outputs/kg_pretrain/${now:%Y-%m-%d}/${now:%H-%M-%S}

defaults:
  - _self_

seed: 1024

datasets:
  _target_: deep_graphrag.datasets.KGDataset
  cfgs:
    root: ./data
    text_emb_model_name: sentence-transformers/all-mpnet-base-v2
  data_names:
    - hotpotqa

model:
  _target_: deep_graphrag.ultra.models.SemanticUltra
  entity_model_cfg:
    _target_: deep_graphrag.ultra.models.EntityNBFNet
    input_dim: 64
    hidden_dims: [64, 64, 64, 64, 64, 64]
    message_func: distmult
    aggregate_func: sum
    short_cut: yes
    layer_norm: yes

task:
  name: TransductiveInference
  num_negative: 256
  strict_negative: yes
  adversarial_temperature: 1
  metric: [mr, mrr, hits@1, hits@3, hits@10]

optimizer:
  _target_: torch.optim.AdamW
  lr: 5.0e-4

train:
  batch_size: 8
  num_epoch: 10
  log_interval: 100
  fast_test: 500
  save_best_only: yes
  batch_per_epoch: null

checkpoint: null
