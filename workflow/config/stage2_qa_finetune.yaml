hydra:
  run:
    dir: outputs/qa_finetune/${now:%Y-%m-%d}/${now:%H-%M-%S}

defaults:
  - _self_
  - doc_ranker: idf_topk_ranker
  - text_emb_model: mpnet

seed: 1024

datasets:
  _target_: deep_graphrag.datasets.QADataset
  cfgs:
    root: ./data
    text_emb_model_cfgs: ${text_emb_model}
  train_names:
    - hotpotqa
  valid_names:
    - hotpotqa_test
    - musique_test
    - 2wikimultihopqa_test

model:
  _target_: deep_graphrag.models.UltraQA
  entity_model:
    _target_: deep_graphrag.ultra.models.QueryNBFNet
    input_dim: 64
    hidden_dims: [64, 64, 64, 64, 64, 64]
    message_func: distmult
    aggregate_func: sum
    short_cut: yes
    layer_norm: yes

task:
  strict_negative: yes
  metric: [mrr, hits@1, hits@2, hits@3, hits@5, hits@10, hits@20, hits@50, hits@100]
  losses:
    - name: ent_bce_loss
      loss:
        _target_: deep_graphrag.losses.BCELoss
        adversarial_temperature: 0.2
      cfg:
        weight: 1.0
        is_doc_loss: False



optimizer:
  _target_: torch.optim.AdamW
  lr: 5.0e-4

train:
  batch_size: 8
  num_epoch: 20
  log_interval: 100
  batch_per_epoch: null
  save_best_only: yes
  save_pretrained: yes # Save the model for QA inference
  do_eval: yes
  timeout: 60 # timeout minutes for multi-gpu training
  init_entities_weight: True

checkpoint: null
